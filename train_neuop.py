from packages import *
from lploss import *
from neu_op import *



# --------------------------------------------- Training file for neu_op.py --------------------------------------------
# This module executes training of the neural operator
# it generates an output file for monitoring losses and other parameters during training
# It also saves the model and losses files every 50 epochs
# This file can be edited and customized by the user
# Places need to be edited by the user are signified by: >>>>> (edit) <<<<<


# >>>>> initial output file (edit) <<<<<
dir_path = os.getcwd()

py_version = platform.python_version()
date_time = datetime.now().strftime("%d-%b-%Y %H:%M:%S")
today = datetime.now().strftime("%m%d%Y")

# >>>>> data file path (edit) <<<<<
data_file_path = "/scratch/user/daniel_88/CMG_SimData/CMG_TP_40x40x366_dt05_TrainVal_4000_280x280x5_2.npy"

output_filename = os.getcwd()+f'/output.txt'
print(output_filename)

f = open(output_filename, "w")
f.write(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   Output File for Neural Operator Project   <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\n")
f.write(f"Daniel Badawi      {date_time}\n")
f.write(f"Python version ({py_version})\n")
f.close()

# get device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

if str(device) == 'cuda':
    f = open(output_filename, "a")
    f.write(f"device: {device}, with total memory {torch.cuda.get_device_properties(0).total_memory}\n")
    f.write(f"GPU name: {torch.cuda.get_device_name(device=device)}\n")
else:
    f = open(output_filename, "a")
    f.write(f"device: {device}\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")

# define units
MPa = 1e6
p0 = 30*MPa
mD = 9.869233e-16

f.write("Units:\n")
f.write("------\n")
f.write(f"MPa: {MPa:.1e}\n")
f.write(f"mD: {mD:.6e} m^2\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")


# import data
with open(data_file_path, "rb") as ff:
    inp_data_original = np.load(ff)
    outp_data_original = np.load(ff)
ff.close()


# this is just to mask very minor roundoff errors generated by the simulator.
outp_data_original[...,1] = np.where(outp_data_original[...,1]<0.2, 0.2, outp_data_original[...,1])


f.write(">>> Input data ...\n")
f.write("Reading input file: "+data_file_path+"\n")
f.write(f"input data shape: {inp_data_original.shape}\n")
f.write(f"target data shape: {outp_data_original.shape}\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")

# define training and testing sets

np.random.seed(0)

ntrain = 3500
ntest = 500

# nt is the number of time snapshots predicted simultaneously
nt = 40

# the first 240 time snapshots are considered for training, and from 0-366 time snapshots are allocated for testing
# this time split is implemented to test extrapolation in time, test samples might have time range between [240,366]
tsplit = 240

f.write(">>> Time range training testing split ...\n")
f.write(f"Training time range: {inp_data_original[0,0,0,:tsplit-nt,-1]} days\n\n")
f.write(f"Testing time range: {inp_data_original[0,0,0,tsplit:,-1]} days\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")

train_a = np.zeros_like(inp_data_original[:ntrain,...,:nt,:])
train_u = np.zeros_like(outp_data_original[:ntrain,...,:nt,:])

test_a = np.zeros_like(inp_data_original[:ntest,...,:nt,:])
test_u = np.zeros_like(outp_data_original[:ntest,...,:nt,:])

# training time indices between (0,240-40), and since we predict 40 timesteps the prediction will cover until 240 snapshots.
tr_indices = np.random.randint(0, tsplit-nt, size=(ntrain,))
# testing time indices between (0,326-40), and since we predict 40 timesteps the prediction will cover until 366 snapshots.
ts_indices = np.random.randint(0, inp_data_original.shape[3]-nt, size=(ntest,))

# intentionally taking 10% of the training and testing samples to be at t=0.
tr_indices[:floor(0.1*ntrain)] = 0
ts_indices[:floor(0.1*ntest)] = 0


for i in range(ntrain):
    train_a[i] = inp_data_original[i, ..., tr_indices[i]:tr_indices[i]+nt, : ]
    train_u[i] = outp_data_original[i, ..., tr_indices[i]:tr_indices[i]+nt, : ]
    
for i in range(ntest):
    test_a[i] = inp_data_original[inp_data_original.shape[0]-ntest+i, ..., ts_indices[i]:ts_indices[i]+nt, : ]
    test_u[i] = outp_data_original[inp_data_original.shape[0]-ntest+i, ..., ts_indices[i]:ts_indices[i]+nt, : ]


f.write(">>> Train-test data split ...\n")
f.write(f"ntrain={ntrain}, ntest={ntest}\n")
f.write(f"train_a shape: {train_a.shape}, test_a shape: {test_a.shape}\n")
f.write(f"train_u shape: {train_a.shape}, test_u shape: {test_a.shape}\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")

# Insert initial pressure to channels
i_pres = train_u[...,0:1,0]
i_sw = train_u[...,0:1,1]
train_a = np.insert(train_a, 4, i_pres, axis=-1)
train_a = np.insert(train_a, 5, i_sw, axis=-1)

i_pres = test_u[...,0:1,0]
i_sw = test_u[...,0:1,1]
test_a = np.insert(test_a, 4, i_pres, axis=-1)
test_a = np.insert(test_a, 5, i_sw, axis=-1)

f.write(">>> Appending initial pressure channel ...\n")
f.write(f"train_a shape: {train_a.shape}, test_a shape: {test_a.shape}\n")
f.write(f"train_u shape: {train_a.shape}, test_u shape: {test_a.shape}\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")


# max time range
max_time = np.max(train_a[...,-1])

f.write(f"training max time: {max_time:.1f} days\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")


# Normalize the permeability based on min/max log10
f.write(">>> Normalize the permeability based on min/max log10 ...\n\n")
kmax = np.amax(train_a[...,0])
kmin = np.amin(train_a[...,0])

pmin = np.amin(train_a[..., 1][train_a[..., 1].nonzero()])
pmax = np.amax(train_u[..., 0])

qmin = np.amin(train_a[..., 2][train_a[..., 2].nonzero()])
qmax = np.amax(train_a[..., 2])

train_a[..., 0] = np.log10(train_a[..., 0]/kmin) / np.log10(kmax/kmin)
test_a[..., 0] = np.log10(test_a[..., 0]/kmin) / np.log10(kmax/kmin)

f.write(">>> Normalize bottomhole pressures and output pressures ...\n\n")

# normalize well controls
train_a[..., 1] = train_a[..., 1] / pmax
test_a[..., 1] = test_a[..., 1] / pmax
train_a[..., 2] = train_a[..., 2] / qmax
test_a[..., 2] = test_a[..., 2] / qmax

# normalize initial condition
train_a[...,4] = train_a[..., 4] / pmax
test_a[...,4] = test_a[..., 4] / pmax

# normalize output pressure
train_u[...,0] = train_u[...,0] / pmax
test_u[...,0] = test_u[...,0] / pmax

f.write(">>> Normalize space and time ...\n\n")
# X
xmin, xmax = train_a[0,0,0,0,-3], train_a[0,-1,0,0,-3]
train_a[..., -3] = (train_a[..., -3] - xmin) / (xmax-xmin)
test_a[..., -3] = (test_a[..., -3] - xmin) / (xmax-xmin)

# Y
ymin, ymax = train_a[0,0,0,0,-2], train_a[0,0,-1,0,-2]
train_a[..., -2] = (train_a[..., -2] - ymin) / (ymax-ymin)
test_a[..., -2] = (test_a[..., -2]  - ymin) / (ymax-ymin)

# T
train_a[..., -1] = train_a[..., -1] / max_time
test_a[..., -1] = test_a[..., -1] / max_time

f.write(">>> Train-Test Split and flip dataset to generate more data samples ...\n\n")

nx = train_a.shape[1]
ny = train_a.shape[2]
nt = train_a.shape[3]

f.write(f"nx={nx}, ny={ny}, nt={nt}\n")


#--------------------------------------------------------------------------
# add augmented images to increase training samples and reduce overfitting
# if you're limited with memory, you can do the augmentation while training
# per batchsize instead of doing the augmentation for the whole dataset at once

augment = True

if augment:
    train_a_flipud = np.flip(train_a, axis=1)
    train_a_fliplr = np.flip(train_a, axis=2)
    train_a_flipdiag = np.flip(train_a_fliplr, axis=1)

    train_u_flipud = np.flip(train_u, axis=1)
    train_u_fliplr = np.flip(train_u, axis=2)
    train_u_flipdiag = np.flip(train_u_fliplr, axis=1)

    train_a = np.concatenate([train_a, train_a_flipud, train_a_fliplr, train_a_flipdiag], axis=0)
    train_u = np.concatenate([train_u, train_u_flipud, train_u_fliplr, train_u_flipdiag], axis=0)
    
    ntrain = 4*ntrain

  
f.write(f"new ntrain={ntrain}, ntest={ntest}\n")
f.write(f"train_a shape: {train_a.shape}, test_a shape: {test_a.shape}\n")
f.write(f"train_u shape: {train_u.shape}, test_u shape: {test_u.shape}\n")
f.write("-------------------------------------------------------------------------------------------------------\n")


# Convert numpy to torch
f.write(">>> Convert numpy to torch ...\n\n")
train_a = torch.from_numpy(train_a).to(torch.float32)
train_u = torch.from_numpy(train_u).to(torch.float32)
test_a = torch.from_numpy(test_a).to(torch.float32)
test_u = torch.from_numpy(test_u).to(torch.float32)
f.write(f"train_a type: {train_a.size()}, test_a type: {test_a.size()}\n")
f.write(f"train_u type: {train_u.size()}, test_u type: {test_u.size()}\n")
f.write("-------------------------------------------------------------------------------------------------------\n")



# derivative function
def calc_derv(x, u):
    """calculates the derivatives dp_x
    and dp_dy on the grid"""
       
    dU_dx, dU_dy = torch.gradient(u, spacing=(x[0,:,0,0,-3], x[0,0,:,0,-2],), dim=(1,2))
    
    return dU_dx, dU_dy
    

# define masked loss
def MyMaskLoss(x, pred_y, true_y, Threshold):

    """this function enables focusing training on saturation plume only"""
    
    dsw_dx, dsw_dy = calc_derv(x, true_y)
    
    dsw_dr = torch.sqrt(dsw_dx**2 + dsw_dy**2)
    
    Mask = torch.abs(dsw_dr) > Threshold

    Masked_pred = torch.where(Mask, pred_y, torch.nan)
    Masked_true = torch.where(Mask, true_y, torch.nan)

    Masked_Norm = torch.sqrt( torch.nansum((Masked_pred-Masked_true)**2, dim=(1,2,3)) ) / torch.sqrt( torch.nansum(Masked_true**2, dim=(1,2,3)) )
    
    norm_sum = torch.sum(Masked_Norm, dim=0)

    return norm_sum


# Instantiate model
mode1 = 10
mode2 = 10
mode3 = 10
width = 36

# -3 because we are not including (x,y,t) VALUES (NOT CHANNELS) in training because the are uniformly distributed, and the requirement is fullfilled by FFT by construction
input_channels = train_a.shape[-1] - 3 
output_channels = train_u.shape[-1]

model = NeuralOperator(input_channels, output_channels, mode1, mode2, mode3, width)
model.to(device)

best_model_state = model.state_dict()
epochs = 800
learning_rate = 0.001

# Lump training and testing data into torch train and test loaders
batch_size = 10
train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u),
                                           batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u),
                                          batch_size=batch_size, shuffle=True)

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)

# step size to drop learning rate
step_size = 100

# instantiate loss
myloss = LpLoss(size_average=False)

# for caching loss values
train_loss = np.empty((0,5))
test_loss = np.empty((0,5))


min_loss = 1e6
beta = 1.0 # derivative multiplier

f.write(">>> Initialize model ...\n\n")
f.write(f"mode1 = {mode1}\nmode2 = {mode2}\nmode3 = {mode3}\nwidth = {width}\ninput_channels = {input_channels}\n")
f.write(f"epochs = {epochs}\nbatch_size = {batch_size}\nlearning_rate = {learning_rate}\noptimizer = {optimizer.__class__.__name__}\n")
f.write(f"scheduler type: custom step\n")
f.write(f"scheduler step size: {step_size}\n")
f.write(f"derivative multiplier = {beta}\n")
f.write("*Note: input_channels are: [perm, prod_bhp, inj_rates, well_locs, p_incon, sw_incon]\n")
f.write("-------------------------------------------------------------------------------------------------------\n")

f.write(">>> Training ...\n\n")
f.close()
time_begin = default_timer()

# initiate model files
LX = int(inp_data_original[0,-1,0,0,-2] + inp_data_original[0,0,0,0,-2])
LY = int(inp_data_original[0,0,-1,0,-3] + inp_data_original[0,0,0,0,-3])


# >>>>> define model and loss file names (edit) <<<<<
loss_file_path = f'/losses_{today}_augmented{ntrain//4}epoch{epochs}.npy'
model_path = f'/model_{today}_augmented{ntrain//4}epoch{epochs}.pt'

os.rename(output_filename, f"outfile_{ntrain}_{today}.txt")
output_filename = os.path.join(os.getcwd(), f"outfile_{ntrain}_{today}.txt")

ep = 0

while ep < epochs:
    
    model.train()
    t1 = default_timer()
    train_l2, train_l2_p, train_l2_sw, train_l2_dp, train_l2_dsw = 0.0, 0.0, 0.0, 0.0, 0.0
    
    
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        
        optimizer.zero_grad()

        pred = model(x[...,:input_channels])
        
        dp_dx, dp_dy = calc_derv(x, y[...,0])
        pred_dpdx, pred_dpdy = calc_derv(x, pred[...,0])
        
        dsw_dx, dsw_dy = calc_derv(x, y[...,1])
        pred_dswdx, pred_dswdy = calc_derv(x, pred[...,1])
        
        p_loss = myloss(pred[...,0], y[...,0])
        sw_loss = MyMaskLoss(x, pred[...,1], y[...,1], 0.01)
        
        dp_loss = torch.sqrt(myloss(pred_dpdx, dp_dx)**2 + myloss(pred_dpdy, dp_dy)**2)
        dsw_loss = torch.sqrt(myloss(pred_dswdx, dsw_dx)**2 + myloss(pred_dswdy, dsw_dy)**2)

        loss = p_loss + sw_loss + beta*dp_loss + beta*dsw_loss
        
        loss.backward()
        optimizer.step()
        
        train_l2 += loss.item()
        train_l2_p += p_loss.item()
        train_l2_sw += sw_loss.item()
        train_l2_dp += dp_loss.item()
        train_l2_dsw += dsw_loss.item()

    model.eval()
    test_l2, test_l2_p, test_l2_sw, test_l2_dp, test_l2_dsw = 0.0, 0.0, 0.0, 0.0, 0.0

    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)

            pred = model(x[...,:input_channels])
                                   
            dp_dx, dp_dy = calc_derv(x, y[...,0])
            pred_dpdx, pred_dpdy = calc_derv(x, pred[...,0])
            
            dsw_dx, dsw_dy = calc_derv(x, y[...,1])
            pred_dswdx, pred_dswdy = calc_derv(x, pred[...,1])
            
            p_loss = myloss(pred[...,0], y[...,0])
            sw_loss = MyMaskLoss(x, pred[...,1], y[...,1], 0.01)
        
            dp_loss = torch.sqrt(myloss(pred_dpdx, dp_dx)**2 + myloss(pred_dpdy, dp_dy)**2)
            dsw_loss = torch.sqrt(myloss(pred_dswdx, dsw_dx)**2 + myloss(pred_dswdy, dsw_dy)**2)

            loss = p_loss + sw_loss + beta*dp_loss + beta*dsw_loss

            test_l2 += loss.item()
            test_l2_p += p_loss.item()
            test_l2_sw += sw_loss.item()
            test_l2_dp += dp_loss.item()
            test_l2_dsw += dsw_loss.item()

    train_l2 /= ntrain
    train_l2_p /= ntrain
    train_l2_sw /= ntrain
    train_l2_dp /= ntrain
    train_l2_dsw /= ntrain
    
    train_loss = np.concatenate((train_loss, np.array([[train_l2, train_l2_p, train_l2_sw, train_l2_dp, train_l2_dsw]])), axis=0)
    
    test_l2 /= ntest
    test_l2_p /= ntest
    test_l2_sw /= ntest
    test_l2_dp /= ntest
    test_l2_dsw /= ntest
    
    test_loss = np.concatenate((test_loss, np.array([[test_l2, test_l2_p, test_l2_sw, test_l2_dp, test_l2_dsw]])), axis=0)

#    if test_loss[-1] < min_loss:
#       best_model_state = copy.deepcopy(model.state_dict())
#       min_loss = test_loss[-1]

    t2 = default_timer()
    
    # write to file every 10 epochs
    if ep%10==0:
        f = open(output_filename, "a")
        f.write(f"epoch: {ep}/{epochs}, time: {t2-t1:.3f} s/epoch, tr_loss: {train_l2:.3e}, val_los: {test_l2:.3e}, lr: {optimizer.param_groups[0]['lr']:.2e}\n")
        f.close()

    # save model every 50 epochs
    if ep%50==0:
        f = open(output_filename, "a")
        f.write(f">>> Saving models, ep={ep}/{epochs}\n")
        f.close()
       
        with open(os.getcwd()+loss_file_path, 'wb') as ff:
            np.save(ff, train_loss)
            np.save(ff, test_loss)
         
        best_model_state = copy.deepcopy(model.state_dict()) 
        torch.save(best_model_state, os.getcwd()+model_path)
        
    ep += 1
    
    if ep%100==0 and ep<600:
        optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / 2


# stop time
time_finish = default_timer()

mins, secs = divmod(ceil(time_finish-time_begin), 60)
hours, mins = divmod(mins, 60)
days, hours = divmod(hours, 24)

# write to file training time
f = open(output_filename, "a")
f.write(">>> Training Executed Successfully!\n")
f.write(f"Total training time: {days:02d}-{hours:02d}:{mins:02d}:{secs:02d}\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")

# write to file losses and model paths
f.write(">>> Saving model and data ...\n")
f.write(f"losses file path: {os.path.join(os.getcwd(),loss_file_path)}\n")
f.write(f"model file path: {os.path.join(os.getcwd(),model_path)}\n")

# save losses
with open(os.path.join(os.getcwd(),loss_file_path), 'wb') as ff:
    np.save(ff, train_loss)
    np.save(ff, test_loss)
ff.close()

# save model
torch.save(best_model_state, os.path.join(os.getcwd(),model_path))

f.write(">>> Model Saved Successfully!\n")
f.write("-------------------------------------------------------------------------------------------------------\n\n")
f.close()
